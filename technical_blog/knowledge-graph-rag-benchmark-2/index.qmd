---
title: "Don't RAG on Knowledge Graphs(Or Do) Benchmarking: Finally Building a Knowledge Graph -- *Part Two*"
date: 2024-04-20
description: Building a knowledge graph in Python with Claude 3 Haiku(Works for ≥ GPT 3.5 as well)
abstract: This post introduces you to building a knowledge graph in Python using an LLM. This involves orchestrating the working components of LangChain in order to call the LLM, compose the prompts, and create our pipeline with its expression language. We then visualize the graph with rustworkx.
aliases:
    - knowledge-graph-benchmark-2
number-sections: true
code-fold: true
categories:
    - knowledge-graphs
    - rag
    - benchmarking
---

**On the last episode of**: [Don't RAG on Knowledge Graphs(Or Do) Benchmarking: Theory behind using an LLM to Build Knowledge Graphs -- *Part One*](../knowledge-graph-rag-benchmark-1/index.qmd):

-   [Claude 3 Haiku](../knowledge-graph-rag-benchmark-1/#jean-claude-van-damme-tell-me-a-haiku) is our model of choice due to it being in the Goldilocks zone of performance and cost.

-   [Relying on large context windows isn't enough](../knowledge-graph-rag-benchmark-1/index.qmd#motivation-for-rag-over-large-context-windows), we need to impart a structure on the data for efficient reuse and robust grounding.

-   When creating a knowledge graph, we need to not only be clever about [controlling our context window](../knowledge-graph-rag-benchmark-1/index.qmd##knowledge-stuffing), but also having a process through which the connections in the [graph can grow](../knowledge-graph-rag-benchmark-1/index.qmd#letting-the-llm-loose).

------------------------------------------------------------------------

**Finally**, we're getting to the fun part. Like many, I thought this day would never come, but here we are.

I'm going to introduce the numerous components we'll be using, and then combine them into our knowledge graph creation pipeline.

# Lets Split Some Text

In order to feed text of reasonable length into our LLM, we need to be able to split it. The splitting criteria will be the token length of the passage. To implement this criterion, we need to create a length function that will be passed into our splitter, and then test it on one of the paragraphs we have available from the MuSiQue dataset.

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#token_len echo=true >}}

As noted in the [last post](../knowledge-graph-rag-benchmark-1#claude-tokenization), we're going to do a little assuming about the Claude 3 Haiku tokenization and say that it's comparable to the latest OpenAI models -- which is why we're going to get away with using OpenAI's tokenizer, tiktoken.

::: callout-note
## As of this writing, Meta's Llama 3 was just released and is using OpenAI's tiktoken (*and it's incredible)*
:::

We'll be using LangChain's `RecursiveCharacterTextSplitter` to split the text into chunks. It algorithmically uses punctuation to help split the text in order to preserve some sentence structure, so sometimes, the chunks will be smaller than our specified chunk size. For illustrative purposes, the following example will use a chunk size and a chunk overlap different from what we'll end up using in the pipeline. Two of the paragraphs are split below with a specified chunk size of 20 and an overlap of 5. If you peek into the code, you can see that we're using our length function as the determinant of splits.

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#split_examples echo=true >}}

# Prompting

Prompting our model is as simple as loading up the API key as an environmental variable, then instantiating the model with Langchain. We can pass in any text string we want to the model as long as it observes the token limits.

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#model_load echo=true >}}

::: column-margin
`dotenv` allows us to load environmental variables from a `.env` file

![](images/dotenv_ex.jpg){width="132"}
:::

It's *so over* for stand up comedians.

While we can easily pass strings into the LLM call, LangChain provides us with templates, which enable endless composability and modularity, as will be witnessed as we create our fairly elaborate prompts – but first, an illustration of the structure we'll be using.

![Prompt Template Composition](images/prompt_structure.png){#fig-prompts .column-page}

As witnessed in the above, we're creating a template out of multiple templates. A `System Message` is a message sent to an LLM that tells it how to respond, in the style, tone, or format of your choosing; it primes it with an 'identity'. The `Human Message` is the message you send to the LLM after you prime it with a system message. Do you actually need to differentiate between them? **Meh**. In my experience it makes no difference and I haven't seen any testing to suggest otherwise, but in the case that future models start to take the distinction more seriously, we should continue using it. LLMs which function as chat models tend to be able to take a series of messages through their APIs, which LangChain is helping us facilitate.

Lets decompose the components of `gen_template`, the main template we'll be using in our pipeline.

::: callout-note
## The difference between a prompt and a template is the fact that a template can contain {*placeholder variables*} which can be replaced in our pipeline, as you will see.
:::

## graph_analyst_template

This is the main system prompt template. It's going to inform the LLM of its purpose, the format we expect it to return to us, the format of what we send to it, and any history we want it to take into account when generating its response.

### Instructions (Pydantic and JSON Schema Magic)

To programatically build a knowledge graph, the output of the LLM will have to be very specific and in a format we can easily process. Foundational models like Claude 3 excel at processing code and various formatted specifications. The specification that's of interest to us is the [JSON Schema](https://json-schema.org), which is designed to describe the structure of JSON data. [Here](https://json-schema.org/learn/json-schema-examples) are some examples of this specification. It describes the fields, their types, and any particular data structures you need in your JSON.

I trust you've perused the examples and are not too stoked to write all of that out yourself. Well, you won't have to because we can express the same thing in a cleaner pythonic format using the [Pydantic library](https://docs.pydantic.dev/latest/) -- it makes structured outputs a breeze. In fact, there are entire libraries, like [Instructor](https://github.com/jxnl/instructor) that are centered on using Pydantic to generate structured output from LLMs that help you validate the output against the schema specification.

The nodes and edges we need to construct for the knowledge graph aren't overly complex, but they do have their nuances and enough moving parts to warrant a systemic approach to their production.

![The node-edge structure we construct from the outputs.](images/node_edge.png){#fig-node-edge .preview-image width="665"}

Each individual node has an identifier, a category, a variable number of attributes, the source text it was created from, and an identifier of the paragraph it was created from taken from the dataset itself. The LLM won't have to generate all of the properties, as the paragraph ID is simply taken from the paragraph that creates it; in fact, it can probably be a list of IDs where that particular node is referenced. The edges are a degree simpler, as they just need a category, some attributes, and the nodes which they connect.

::: callout-tip
## Pydantic, along with a similar sort of workflow can be generalized for structured extraction of any sort with LLMs. You define the JSON structure, feed the LLM a passage, and it extracts the fields you specified. This is a complete game-changer for machine learning and feature generation(much more exciting than chatbots, IMO).
:::

Below, you'll see each class represent a distinct JSON object, with the fields and instructions that the model will receive. By using the `BaseModel` superclass😎, we can create Pydantic classes with the following syntax:

::: {style="max-height: 800px; overflow: auto"}
{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#pydantic echo=true >}}
:::

The Graph class is the ultimate class we're using to generate the JSON schema. It combines the Node and Edge classes into lists, as we want the final output to be a collection of nodes and the edges that connect them. `model_json_schema()` outputs the JSON schema of the format we want the LLM to return.

It may be worthwhile to read through the fields and their descriptions carefully, and mind the `semantic_id` in the `Node` class; its purpose is to allow the LLM to use that identifier in the `from_node` and `to_node` fields of the edges.

::: callout-note
## You can *probably* use Pydantic classes to describe the JSON output we need without even generating the JSON schema. Such is the magic of LLMs.
:::

In addition to our fancy JSON schema generated with Pydantic, which already includes some descriptions of the fields, we need to pass in some instructions.

::: {style="max-height: 500px; overflow: auto"}
{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#json_instructions echo=true >}}
:::

This prompt states that if a history of nodes and edges is provided, then the LLM is at liberty to reuse those semantic ids in order to modify their respective nodes and edges. Doing this allows for the knowledge graph to grow more dynamically as it processes more information.

For example, if we have two separate chunks of text that the LLM is exposed to at different times, considering that there is some adjacency between the processing of the passages, since we won't keep the entire history of nodes and edges in the context window.

> Fido ran over the bridge

and

> Fido was hungry and stole a donut.

The `semantic_id` that identifies Fido would persist, so that the particular entity wouldn't be duplicated.

![The semantic id allows for continuity of the entity 'Fido'](images/fido.png){#fig-fido width="526"}

### Content

In addition to the JSON formatting instructions, we give the model some high-level guidance. The placeholders are included as `{instructions}` where the previously constructed JSON instructions will go, and `history` where past nodes and edges will be inserted – the format isn't critical, but we'll stick to the JSON schema we're using for the output.

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#graph_creator echo=true >}}

## pass_passage_template

The human message portion of this template consists of something as simple as:

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#pass_passage echo=true >}}

where `{passage]` is our placeholder for the chunk(s) of text we grab from our paragraphs.

## Combining the Prompt Templates

To create our Langchain pipeline, we wrap the templates we created in `SystemMessagePromptTemplate` and `HumanMessagePromptTemplate` classes, and then combine them into `gen_template`.

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#template_agg echo=true >}}

`invoke` is a generic command in Langchain's expression language(LCEL) which can be applied to many of the Langchain elements in order to 'trigger' them. This makes the interface quite simple when building chains of elements and extending the types of elements that are available to your custom chains by implementing your own classes that contain the `invoke` method(and others).

::: callout-note
## Generally, we can use [partial_variables](https://python.langchain.com/docs/modules/model_io/prompts/partial/) within the prompt templates in order to not have to pass in the json_prompt_instructions on each invocation -- but a recent Langchain update(langchain == 0.1.16) did us wrong and broke that for quite a few templates.
:::

# Knowledge Graph Generation (Without History)

We now, more or less, have the components necessary to give knowledge graph generation a first go. Development is generally iterative so we'll leave out the history aspect of it for the time being.

We now will take a gander at the LCEL in action:

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#test1 echo=true >}}

That's all there is to it. We pipe(`|`) the output from invoking the `gen_template` straight to the `chat_model` which also gets invoked.

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#test2 echo=true >}}

`llm_pipe` is passed the same arguments that `gen_template` would've been.

::: {style="max-height: 400px; overflow: auto"}
{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#response_contents echo=true >}}
:::

Would you look at that, it did what we told it to, and it cost less than a penny. However, it's still a string, so we need to convert it into a more amiable format.

::: {#parsed_output_example style="max-height: 400px; overflow: auto"}
{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#json_parsed echo=true >}}
:::

Using Langchain's `JsonOutputParser` allows us to easily convert the JSON string into a Python dictionary object. We're once again calling `invoke` which means it could easily be inserted into our pipeline:

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#whole_pipe echo=true >}}

Before assuming that the output would be correctly structured JSON string, we needed to see it for ourselves. If the output from `gen_template | chat_model` ended up anything else other than a JSON string that our parser can handle, we would've received an unfortunate error.

Generally speaking, if you have a prompt that plays ball with an LLM of your choosing, you're fairly safe when it comes to receiving the structured output in the subsequent calls. However, it is a best practice to involve a failsafe that can retry the process in the even of failure. The failsafe method can involve something as simple as sending the faulty output along with a string that describes your desired output back into the LLM for re-evaluation. For instance:

```         
You didn't output the proper JSON format. Please try again.
This was your output:
{output}
```

We can skip that for now, and see how robust our pipeline really is. Risk it for the biscuit. <sub><sup>🙏</sup></sub>

## Visualization with rustworkx {#visualization-with-rustworkx}

The easiest way to visualize our newly-formed knowledge graph is by using a network graph library; in our case, I've chosen [rustworkx](https://www.rustworkx.org/index.html). It's a Python library that allows for the creation, manipulation, and rendering of directed graphs. If you're familiar with networkx, then the syntax will be very similar, however the performance is a magnitude faster given that all of the internal goodies are written in Rust.

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#base_graph_viz echo=true >}}

It ain't pretty, but it's honest work.

To create the graph, we use a dictionary to map the node `semantic_id` to the generated node index which is output when we create a new node. Then to create edges, that mapping is used to convert the `semantic_id` to the index.

# Knowledge Graph Generation (With History)

## History Management

When it comes to managing the history of nodes and edges, there is a tiny bit of overhead involved. We need to:

-   Keep track of the generated nodes and edges and thusly provide them with unique identifiers

-   Add new edges and nodes to the history

-   Update edges and nodes if the LLM makes changes to them

-   Return a string representation of the nodes and edges to our pipeline using a specified token limit dependent on the context size

To do this, we will create a magnificent `GraphHistory` class that manages this storage and retrieval.

(Unfolding the code not for the faint of heart)

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#history_class echo=true >}}

The code above addresses the bullets representing our requirements, however, there are some messy workarounds where we skip creating edges when it imagines node names. The ideal handling of this would involve rerunning the generation and feeding it the error, but we're going to wing it and skip this for better or worse. The mistaken identity shouldn't be very common, but it *can* occur.

# Putting it All Together

Now that we have an ability to store and inject history into our pipeline, we're ready to go.

We're iterating over all of the paragraphs, and then splitting each paragraph with the `RecursiveCharacterTextSplitter`.

Some things to note about our new pipeline before you dive in:

1.  The JSON parser is now wrapped with a special `OutputFixingParser` class from Langchain that in the event of an error like a `JSONDecodeError`, it sends that error back to the LLM and tries to generate the correct format. Experimenting with Claude 3 Haiku has led me to add that, as it had generated faulty JSON(unlike GPT 3.5). This gives more credence to the user stories claiming that Claude 3 is more buddy-buddy with XML over JSON.
2.  A way to handle the `RateLimitError` exception was added, in the event that the [API complains](https://docs.anthropic.com/claude/reference/rate-limits) when we generate too many nodes and edges back to back. All it takes is waiting a minute before retrying.
3.  The `paragraph_idx` is added to the nodes to indicate which paragraph it was generated from.
4.  The nodes and edges we generate are stored in `graph_history`, which is a list of objects similar to what we generated [here](#parsed_output_example), but with UUIDs for unique identification(the `semantic_id` generated may, by chance alone, be the same)

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#history_pipe echo=true >}}

::: {style="max-height: 400px; overflow: auto"}
{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#graph_components echo=true >}}
:::

It took \~2 minutes to create generate the `graph_history`and 43 calls to the LLM.

A speed boost can definitely be had in multiple ways:

-   Use a different model. The latest Llama 3 model running on [Groq](https://wow.groq.com) infrastructure can yield a 10x speed up in some cases if you use the 8B model. The great thing about using a framework like Langchain is the ease with which you can plug n play different models in your pipelines.

-   Increase the chunk size. If the entire paragraph is passed to the LLM, this will cut down on the 43 multiple calls by roughly half in our case.

-   In addition to increasing the chunk size, we can pass multiple paragraphs to process at the same time – although this would involve prompting the model to extract some paragraph identification which we currently get for free simply by attaching the `idx` of each paragraph to the nodes it creates.

-   **PARALLELIZE IT**(though we may lose some of the history tracking)

## Show Me the Money

Or the knowledge graph. Back to rustworkx we go. Some minor tweaks were made to the visualization code we [saw earlier](#visualization-with-rustworkx) in order for all of the nodes and edges to not be concealed by the *massive* amounts of text we've generated. I've left the node categories visible. The graph generation code was modified to work with the the history stored within our `graph_history` object.

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#paragraphs_viz echo=true >}}

Messy, but you get the idea.

We can do a little graph analytics to find the most connected nodes(nodes with the most connections). `incident_edges(n)` identifies the edges of a node with index `n`, so all we have to do is get the length of the edge list returned and then sort it.

::: {style="max-height: 400px; overflow: auto"}
{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#node_incident_edges echo=true >}}
:::

The most connections a single node has is 9, while most nodes merely have a single connection, and a minority of nodes have no connections. Here is said node:

{{< embed ../knowledge-graph-rag-benchmark-1/notebooks/kg_build.ipynb#chad_nine echo=true >}}

Do recall, our token window is fairly small, with all of 70 tokens, however, we're using 600 tokens to store the history of generated nodes and edges which are fed back to the model. Perhaps this a good amount of connectivity given these parameters and the fact that only a few paragraphs should have a connection between them, or perhaps not. The lack of quantifiable best practices in a bleeding edge field is 😢sad but expected.

To jog your memory a bit, here is what 70 tokens looks like:

![An example text consisting of 70 tokens](images/70_tokens.jpg){#fig-tokens width="637"}

# Wrapping Up

![](images/thats_all.gif)

Well...**almost!**

Now that we have a workflow for generating knowledge graphs for questions in the MuSiQue dataset, we can move on to attaching a vector database to it in the **next post**.

Thanks for reading, I hope you managed to stay awake.

[**Part Three \>\>\>**](../knowledge-graph-rag-benchmark-3/index.qmd)